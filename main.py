#!/usr/bin/env python3
"""
AST (Audio Spectrogram Transformer) Sound Event Detection API - Supabase Integration
file_paths-based processing with audio_files table integration

Model: MIT/ast-finetuned-audioset-10-10-0.4593
Sampling Rate: 16kHz
Library: transformers (Hugging Face)
"""

import os
import io
import json
import tempfile
import traceback
from typing import List, Dict, Optional
from datetime import datetime, timezone
import time

import torch
import numpy as np
import librosa
import soundfile as sf
from transformers import AutoFeatureExtractor, ASTForAudioClassification
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# AWS S3 and Supabase
import boto3
from botocore.exceptions import ClientError
from supabase import create_client, Client
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Global variables for model
model = None
feature_extractor = None
id2label = None

# Model information
MODEL_NAME = "MIT/ast-finetuned-audioset-10-10-0.4593"
MODEL_DESCRIPTION = "Audio Spectrogram Transformer - AudioSet (mAP: 0.459)"
SAMPLING_RATE = 16000

# Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
supabase_url = os.getenv('SUPABASE_URL')
supabase_key = os.getenv('SUPABASE_KEY')

if not supabase_url or not supabase_key:
    raise ValueError("SUPABASE_URLãŠã‚ˆã³SUPABASE_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

supabase: Client = create_client(supabase_url, supabase_key)
print(f"âœ… Supabaseæ¥ç¶šè¨­å®šå®Œäº†: {supabase_url}")

# AWS S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
s3_bucket_name = os.getenv('S3_BUCKET_NAME', 'watchme-vault')
aws_region = os.getenv('AWS_REGION', 'ap-southeast-2')

if not aws_access_key_id or not aws_secret_access_key:
    raise ValueError("AWS_ACCESS_KEY_IDãŠã‚ˆã³AWS_SECRET_ACCESS_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

s3_client = boto3.client(
    's3',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=aws_region
)
print(f"âœ… AWS S3æ¥ç¶šè¨­å®šå®Œäº†: ãƒã‚±ãƒƒãƒˆ={s3_bucket_name}, ãƒªãƒ¼ã‚¸ãƒ§ãƒ³={aws_region}")

# FastAPI application
app = FastAPI(
    title="AST Audio Event Detection API with Supabase",
    description="Audio Spectrogram Transformer for sound event detection (Supabase integration) - v3",
    version="3.0.0"
)

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã®è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class FetchAndProcessPathsRequest(BaseModel):
    file_paths: List[str]
    threshold: Optional[float] = 0.1
    top_k: Optional[int] = 3
    analyze_timeline: Optional[bool] = True
    segment_duration: Optional[float] = 10.0  # 10ç§’ãŒæœ€é©
    overlap: Optional[float] = 0.0  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãªã—ãŒæœ€é©

def load_model():
    """Load AST model and feature extractor"""
    global model, feature_extractor, id2label

    print(f"ğŸ”„ Loading model: {MODEL_NAME}")
    try:
        feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)
        model = ASTForAudioClassification.from_pretrained(MODEL_NAME)

        # Get label mapping
        id2label = model.config.id2label

        # Set device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model.eval()

        print(f"âœ… Model loaded successfully")
        print(f"   - Model: {MODEL_NAME}")
        print(f"   - Device: {device}")
        print(f"   - Classes: {len(id2label)} (AudioSet)")
        print(f"   - Sampling Rate: {SAMPLING_RATE} Hz (16kHz)")
        print(f"   - Performance: mAP 0.459 (AudioSet)")

    except Exception as e:
        print(f"âŒ Failed to load model: {str(e)}")
        traceback.print_exc()
        raise

def extract_info_from_file_path(file_path: str) -> Dict[str, str]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ãƒã‚¤ã‚¹IDã€æ—¥ä»˜ã€æ™‚é–“ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º

    Args:
        file_path: S3ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ä¾‹: files/device-id/2025-07-20/14-30/audio.wav)

    Returns:
        device_id, date, time_block ã‚’å«ã‚€è¾æ›¸
    """
    parts = file_path.split('/')

    if len(parts) >= 2:
        device_id = parts[1]
        return {
            'device_id': device_id
        }
    else:
        return {
            'device_id': 'unknown'
        }

async def update_audio_files_status(file_path: str, status: str = 'completed'):
    """
    audio_filesãƒ†ãƒ¼ãƒ–ãƒ«ã®behavior_features_statusã‚’æ›´æ–°

    Args:
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        status: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ ('pending', 'processing', 'completed', 'error')
    """
    try:
        update_response = supabase.table('audio_files') \
            .update({'behavior_features_status': status}) \
            .eq('file_path', file_path) \
            .execute()

        if update_response.data:
            print(f"âœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°æˆåŠŸ: {file_path} -> {status}")
            return True
        else:
            print(f"âš ï¸ å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return False

    except Exception as e:
        print(f"âŒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

async def save_to_spot_features(device_id: str, recorded_at: str,
                                 timeline_data: List[Dict]):
    """
    spot_featuresãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã®çµæœã‚’ä¿å­˜

    Args:
        device_id: ãƒ‡ãƒã‚¤ã‚¹ID
        recorded_at: éŒ²éŸ³æ—¥æ™‚ (UTC timestamp)
        timeline_data: ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
    """
    try:
        processed_at = datetime.now(timezone.utc).isoformat()

        # Get local_date and local_time from audio_files table
        local_date = None
        local_time = None
        try:
            audio_file_response = supabase.table('audio_files').select('local_date, local_time').eq(
                'device_id', device_id
            ).eq(
                'recorded_at', recorded_at
            ).execute()

            if audio_file_response.data and len(audio_file_response.data) > 0:
                local_date = audio_file_response.data[0].get('local_date')
                local_time = audio_file_response.data[0].get('local_time')
                print(f"Retrieved local_date from audio_files: {local_date}")
                print(f"Retrieved local_time from audio_files: {local_time}")
            else:
                print(f"âš ï¸ No audio_files record found for device_id={device_id}, recorded_at={recorded_at}")
        except Exception as e:
            print(f"âŒ Error fetching local_date/local_time from audio_files: {e}")

        data = {
            'device_id': device_id,
            'recorded_at': recorded_at,
            'local_date': local_date,  # Local date from audio_files
            'local_time': local_time,  # Local time from audio_files
            'behavior_extractor_result': timeline_data  # JSONBå½¢å¼
        }

        response = supabase.table('spot_features') \
            .upsert(data) \
            .execute()

        if response.data:
            print(f"âœ… spot_featuresä¿å­˜æˆåŠŸ: {device_id}/{recorded_at}")
            return True
        else:
            print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ã™")
            return False

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        traceback.print_exc()
        return False

def download_from_s3(file_path: str, local_path: str) -> bool:
    """S3ã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        print(f"ğŸ“¥ S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {file_path}")
        s3_client.download_file(s3_bucket_name, file_path, local_path)
        print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {file_path}")
        return True
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == '404':
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        else:
            print(f"âŒ S3ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {error_code} - {str(e)}")
        return False
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def process_audio(audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
    """
    Preprocess audio data for AST model

    Args:
        audio_data: Audio data (numpy array)
        sample_rate: Original sampling rate

    Returns:
        Processed audio data
    """
    # Convert to mono
    if len(audio_data.shape) > 1:
        audio_data = np.mean(audio_data, axis=1)

    # Resample to model's expected sampling rate (16kHz)
    target_sr = feature_extractor.sampling_rate
    if sample_rate != target_sr:
        audio_data = librosa.resample(
            audio_data,
            orig_sr=sample_rate,
            target_sr=target_sr
        )

    # Convert to float32
    if audio_data.dtype != np.float32:
        audio_data = audio_data.astype(np.float32)

    # Normalize (-1.0 to 1.0)
    max_val = np.max(np.abs(audio_data))
    if max_val > 0:
        audio_data = audio_data / max_val

    return audio_data

def predict_audio_events(audio_data: np.ndarray, top_k: int = 5,
                        threshold: float = 0.1) -> List[Dict]:
    """
    Predict audio events from audio data

    Args:
        audio_data: Preprocessed audio data
        top_k: Number of top predictions to return
        threshold: Minimum probability threshold

    Returns:
        List of predicted events
    """
    # Extract features
    inputs = feature_extractor(
        audio_data,
        sampling_rate=feature_extractor.sampling_rate,
        return_tensors="pt"
    )

    # Move to device
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Run inference
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # Convert to probabilities
    probs = torch.nn.functional.softmax(logits, dim=-1)[0]

    # Get top-k predictions
    top_probs, top_indices = torch.topk(probs, min(top_k, len(probs)))

    # Format results
    predictions = []
    for prob, idx in zip(top_probs.cpu(), top_indices.cpu()):
        score = prob.item()
        if score >= threshold:
            label_id = idx.item()
            label = id2label.get(label_id) or id2label.get(str(label_id)) or f"Event_{label_id}"
            predictions.append({
                "label": label,
                "score": round(score, 4)
            })

    return predictions

def analyze_timeline(audio_data: np.ndarray, sample_rate: int,
                    segment_duration: float = 10.0,
                    overlap: float = 0.0,
                    top_k: int = 3,
                    threshold: float = 0.1) -> Dict:
    """
    Analyze audio data in timeline segments

    Args:
        audio_data: Audio data
        sample_rate: Sampling rate
        segment_duration: Segment length in seconds (default 10s)
        overlap: Overlap ratio (0-1, default 0)
        top_k: Number of events to return per segment
        threshold: Minimum probability threshold

    Returns:
        Timeline analysis results
    """
    # Preprocess audio
    processed_audio = process_audio(audio_data, sample_rate)
    target_sr = feature_extractor.sampling_rate

    # Segment configuration
    segment_samples = int(segment_duration * target_sr)
    hop_samples = int(segment_samples * (1 - overlap))

    # Store timeline results
    timeline = []
    all_events = {}

    # Handle short audio (less than segment_duration)
    if len(processed_audio) < segment_samples:
        events = predict_audio_events(processed_audio, top_k, threshold)
        timeline.append({
            "time": 0.0,
            "events": events
        })
        for event in events:
            label = event["label"]
            if label not in all_events:
                all_events[label] = {"count": 0, "total_score": 0}
            all_events[label]["count"] += 1
            all_events[label]["total_score"] += event["score"]
    else:
        # Normal segment processing
        for i in range(0, len(processed_audio) - segment_samples + 1, hop_samples):
            segment = processed_audio[i:i + segment_samples]
            time_position = i / target_sr

            # Predict events for segment
            events = predict_audio_events(segment, top_k, threshold)

            # Add to timeline
            timeline.append({
                "time": round(time_position, 1),
                "events": events
            })

            # Aggregate events
            for event in events:
                label = event["label"]
                if label not in all_events:
                    all_events[label] = {"count": 0, "total_score": 0}
                all_events[label]["count"] += 1
                all_events[label]["total_score"] += event["score"]

    # Get most common events
    most_common = []
    for label, stats in sorted(all_events.items(), key=lambda x: x[1]["count"], reverse=True)[:5]:
        most_common.append({
            "label": label,
            "occurrences": stats["count"],
            "average_score": round(stats["total_score"] / stats["count"], 4)
        })

    return {
        "timeline": timeline,
        "summary": {
            "total_segments": len(timeline),
            "duration_seconds": round(len(processed_audio) / target_sr, 1),
            "segment_duration": segment_duration,
            "overlap": overlap,
            "most_common_events": most_common
        }
    }

async def process_single_file(file_path: str, threshold: float = 0.1, top_k: int = 3,
                             analyze_timeline_flag: bool = True,
                             segment_duration: float = 10.0,
                             overlap: float = 0.0) -> Dict:
    """
    å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã§ä¿å­˜ï¼‰
    """
    temp_file = None
    try:
        # audio_filesãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰recorded_atã‚’å–å¾—
        audio_file_response = supabase.table('audio_files') \
            .select('device_id, recorded_at') \
            .eq('file_path', file_path) \
            .single() \
            .execute()

        if not audio_file_response.data:
            return {"status": "error", "file_path": file_path, "error": "Audio file record not found"}

        device_id = audio_file_response.data['device_id']
        recorded_at = audio_file_response.data['recorded_at']

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å‡¦ç†ä¸­ã«æ›´æ–°
        await update_audio_files_status(file_path, 'processing')

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
            temp_file = tmp.name

        if not download_from_s3(file_path, temp_file):
            await update_audio_files_status(file_path, 'error')
            return {"status": "error", "file_path": file_path, "error": "Download failed"}

        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        audio_data, sample_rate = sf.read(temp_file)
        print(f"ğŸµ éŸ³å£°ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(audio_data)/sample_rate:.2f}ç§’, {sample_rate}Hz")

        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åˆ†æã‚’å®Ÿè¡Œ
        timeline_result = analyze_timeline(
            audio_data, sample_rate,
            segment_duration, overlap, top_k, threshold
        )

        # spot_featuresãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
        save_success = await save_to_spot_features(
            device_id,
            recorded_at,
            timeline_result['timeline']
        )

        if save_success:
            await update_audio_files_status(file_path, 'completed')
            return {
                "status": "success",
                "file_path": file_path,
                "device_id": device_id,
                "recorded_at": recorded_at,
                "timeline": timeline_result
            }
        else:
            await update_audio_files_status(file_path, 'error')
            return {"status": "error", "file_path": file_path, "error": "Save failed"}

    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        traceback.print_exc()
        await update_audio_files_status(file_path, 'error')
        return {"status": "error", "file_path": file_path, "error": str(e)}

    finally:
        if temp_file and os.path.exists(temp_file):
            os.remove(temp_file)

@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    load_model()

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "AST Audio Event Detection API with Supabase Integration",
        "model": MODEL_NAME,
        "version": "3.0.0",
        "sampling_rate": f"{SAMPLING_RATE} Hz (16kHz)",
        "status": "ready" if model is not None else "not ready",
        "endpoints": {
            "/fetch-and-process-paths": "Process audio files from S3 via file paths",
            "/health": "Health check endpoint"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy" if model is not None else "unhealthy",
        "model_loaded": model is not None,
        "model_name": MODEL_NAME,
        "sampling_rate": SAMPLING_RATE,
        "supabase_connected": supabase is not None,
        "s3_connected": s3_client is not None
    }

@app.post("/fetch-and-process-paths")
async def fetch_and_process_paths(request: FetchAndProcessPathsRequest):
    """
    file_pathsãƒ™ãƒ¼ã‚¹ã®éŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆv2å®Œå…¨äº’æ›ï¼‰

    Args:
        request: file_pathsé…åˆ—ã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
        å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ã¨è©³ç´°
    """
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    start_time = time.time()

    processed_files = []
    error_files = []

    print(f"ğŸš€ å‡¦ç†é–‹å§‹: {len(request.file_paths)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")

    for file_path in request.file_paths:
        result = await process_single_file(
            file_path,
            request.threshold,
            request.top_k,
            request.analyze_timeline,
            request.segment_duration,
            request.overlap
        )

        if result["status"] == "success":
            processed_files.append(file_path)
        else:
            error_files.append({
                "file_path": file_path,
                "error": result.get("error", "Unknown error")
            })

    execution_time = time.time() - start_time

    total_files = len(request.file_paths)
    success_count = len(processed_files)
    error_count = len(error_files)

    response = {
        "status": "success" if error_count == 0 else "partial",
        "summary": {
            "total_files": total_files,
            "processed": success_count,
            "errors": error_count
        },
        "processed_files": processed_files,
        "error_files": error_files if error_files else None,
        "execution_time_seconds": round(execution_time, 1),
        "message": f"{total_files}ä»¶ä¸­{success_count}ä»¶ã‚’æ­£å¸¸ã«å‡¦ç†ã—ã¾ã—ãŸ"
    }

    print(f"âœ… å‡¦ç†å®Œäº†: {success_count}/{total_files}ä»¶æˆåŠŸ (å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’)")

    return JSONResponse(content=response)

if __name__ == "__main__":
    print("=" * 50)
    print("AST Audio Event Detection API with Supabase")
    print(f"Model: {MODEL_NAME}")
    print(f"Sampling Rate: {SAMPLING_RATE} Hz (16kHz)")
    print("=" * 50)

    uvicorn.run(
        app,
        host="127.0.0.1",
        port=8017,
        log_level="info"
    )
