#!/usr/bin/env python3
"""
PaSST (Patchout Spectrogram Transformer) éŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºAPI - Supabaseçµ±åˆç‰ˆ
file_pathsãƒ™ãƒ¼ã‚¹ã®å‡¦ç†ã§audio_filesãƒ†ãƒ¼ãƒ–ãƒ«ã¨é€£æº

=============================================================================
ğŸ”Š é‡è¦: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã®é•ã„ (v2 AST â†’ v3 PaSST)
=============================================================================

ã€v2 (AST)ã€‘
- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: 16kHz (16000 Hz)
- ãƒ¢ãƒ‡ãƒ«: MIT/ast-finetuned-audioset-10-10-0.4593
- ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: transformers (Hugging Face)

ã€v3 (PaSST)ã€‘
- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: 32kHz (32000 Hz) âš ï¸ v2ã®2å€
- ãƒ¢ãƒ‡ãƒ«: passt_s_swa_p16_128_ap476
- ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: hear21passt

ã€ãªãœ32kHzãªã®ã‹ï¼Ÿã€‘
PaSSTãƒ¢ãƒ‡ãƒ«ã¯å­¦ç¿’æ™‚ã«32kHzã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€
æ¨è«–æ™‚ã‚‚32kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ã€å½±éŸ¿ç¯„å›²ã€‘
- å…¥åŠ›éŸ³å£°ãŒä½•Hzã§ã‚ã£ã¦ã‚‚ã€å†…éƒ¨ã§è‡ªå‹•çš„ã«32kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã¾ã™
- ãƒ¦ãƒ¼ã‚¶ãƒ¼å´ã§ç‰¹åˆ¥ãªå¯¾å¿œã¯ä¸è¦ï¼ˆAPIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯å¤‰æ›´ãªã—ï¼‰
- å‡¦ç†æ™‚é–“ã¯ã»ã¼åŒã˜ï¼ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯è»½å¾®ï¼‰

ã€ç²¾åº¦å‘ä¸Šã€‘
32kHzã«ã‚ˆã‚Šã€ã‚ˆã‚Šé«˜å‘¨æ³¢æ•°å¸¯åŸŸã®éŸ³éŸ¿ç‰¹å¾´ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã€
çµæœã¨ã—ã¦éŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºç²¾åº¦ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ (mAP 0.459 â†’ 0.476)

=============================================================================
"""

import os
import io
import json
import tempfile
import traceback
from typing import List, Dict, Optional
from datetime import datetime, timezone
import time
import ssl

# SSLè¨¼æ˜æ›¸ã®æ¤œè¨¼ã‚’ç„¡åŠ¹åŒ–ï¼ˆé–‹ç™ºç’°å¢ƒã®ã¿ï¼‰
ssl._create_default_https_context = ssl._create_unverified_context
os.environ['PYTHONHTTPSVERIFY'] = '0'

import torch
import numpy as np
import librosa
import soundfile as sf
from hear21passt.base import get_basic_model
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# AWS S3ã¨Supabase
import boto3
from botocore.exceptions import ClientError
from supabase import create_client, Client
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒ
model = None
device = None
labels_map = None  # AudioSetãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°

# ãƒ¢ãƒ‡ãƒ«æƒ…å ±
MODEL_NAME = "PaSST-S SWA (passt_s_swa_p16_128_ap476)"
MODEL_DESCRIPTION = "Patchout Spectrogram Transformer - AudioSet (mAP: 0.476)"
SAMPLING_RATE = 32000  # âš ï¸ v2ã®16kHzã‹ã‚‰32kHzã«å¤‰æ›´

# Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
supabase_url = os.getenv('SUPABASE_URL')
supabase_key = os.getenv('SUPABASE_KEY')

if not supabase_url or not supabase_key:
    raise ValueError("SUPABASE_URLãŠã‚ˆã³SUPABASE_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

supabase: Client = create_client(supabase_url, supabase_key)
print(f"âœ… Supabaseæ¥ç¶šè¨­å®šå®Œäº†: {supabase_url}")

# AWS S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
s3_bucket_name = os.getenv('S3_BUCKET_NAME', 'watchme-vault')
aws_region = os.getenv('AWS_REGION', 'ap-southeast-2')

if not aws_access_key_id or not aws_secret_access_key:
    raise ValueError("AWS_ACCESS_KEY_IDãŠã‚ˆã³AWS_SECRET_ACCESS_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

s3_client = boto3.client(
    's3',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=aws_region
)
print(f"âœ… AWS S3æ¥ç¶šè¨­å®šå®Œäº†: ãƒã‚±ãƒƒãƒˆ={s3_bucket_name}, ãƒªãƒ¼ã‚¸ãƒ§ãƒ³={aws_region}")

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(
    title="PaSST Audio Event Detection API with Supabase",
    description="Patchout Spectrogram Transformer ã‚’ä½¿ç”¨ã—ãŸéŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºAPIï¼ˆSupabaseçµ±åˆç‰ˆï¼‰- v3",
    version="3.0.0"
)

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã®è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class FetchAndProcessPathsRequest(BaseModel):
    file_paths: List[str]
    threshold: Optional[float] = 0.1
    top_k: Optional[int] = 3
    analyze_timeline: Optional[bool] = True
    segment_duration: Optional[float] = 10.0  # 10ç§’ãŒæœ€é©
    overlap: Optional[float] = 0.0  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãªã—ãŒæœ€é©

def load_audioset_labels():
    """AudioSetãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’JSONã‹ã‚‰èª­ã¿è¾¼ã‚€"""
    global labels_map

    try:
        labels_file = os.path.join(os.path.dirname(__file__), 'audioset_labels.json')
        with open(labels_file, 'r', encoding='utf-8') as f:
            labels_map = json.load(f)
        print(f"âœ… AudioSetãƒ©ãƒ™ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(labels_map)}ã‚¯ãƒ©ã‚¹")
    except Exception as e:
        print(f"âš ï¸ AudioSetãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {str(e)}")
        labels_map = {}

def load_model():
    """PaSSTãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    global model, device

    print(f"ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {MODEL_NAME}")
    try:
        # PaSSTãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆlogitsãƒ¢ãƒ¼ãƒ‰ = 527ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
        model = get_basic_model(mode="logits")

        # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        model.eval()

        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ã¾ã—ãŸ")
        print(f"   - ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}")
        print(f"   - ãƒ‡ãƒã‚¤ã‚¹: {device}")
        print(f"   - ã‚¯ãƒ©ã‚¹æ•°: 527 (AudioSet)")
        print(f"   - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: {SAMPLING_RATE} Hz (32kHz)")
        print(f"   - æ€§èƒ½: mAP 0.476 (AudioSet)")

        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã‚€
        load_audioset_labels()

    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        traceback.print_exc()
        raise

def extract_info_from_file_path(file_path: str) -> Dict[str, str]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ãƒã‚¤ã‚¹IDã€æ—¥ä»˜ã€æ™‚é–“ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º

    Args:
        file_path: S3ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ä¾‹: files/device-id/2025-07-20/14-30/audio.wav)

    Returns:
        device_id, date, time_block ã‚’å«ã‚€è¾æ›¸
    """
    parts = file_path.split('/')

    if len(parts) >= 2:
        device_id = parts[1]
        return {
            'device_id': device_id
        }
    else:
        return {
            'device_id': 'unknown'
        }

async def update_audio_files_status(file_path: str, status: str = 'completed'):
    """
    audio_filesãƒ†ãƒ¼ãƒ–ãƒ«ã®behavior_features_statusã‚’æ›´æ–°

    Args:
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        status: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ ('pending', 'processing', 'completed', 'error')
    """
    try:
        update_response = supabase.table('audio_files') \
            .update({'behavior_features_status': status}) \
            .eq('file_path', file_path) \
            .execute()

        if update_response.data:
            print(f"âœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°æˆåŠŸ: {file_path} -> {status}")
            return True
        else:
            print(f"âš ï¸ å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return False

    except Exception as e:
        print(f"âŒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

async def save_to_spot_features(device_id: str, recorded_at: str,
                                 timeline_data: List[Dict]):
    """
    spot_featuresãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã®çµæœã‚’ä¿å­˜

    Args:
        device_id: ãƒ‡ãƒã‚¤ã‚¹ID
        recorded_at: éŒ²éŸ³æ—¥æ™‚ (UTC timestamp)
        timeline_data: ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
    """
    try:
        processed_at = datetime.now(timezone.utc).isoformat()

        data = {
            'device_id': device_id,
            'recorded_at': recorded_at,
            'behavior_extractor_result': timeline_data,  # JSONBå½¢å¼
            'behavior_extractor_status': 'completed',
            'behavior_extractor_processed_at': processed_at
        }

        response = supabase.table('spot_features') \
            .upsert(data) \
            .execute()

        if response.data:
            print(f"âœ… spot_featuresä¿å­˜æˆåŠŸ: {device_id}/{recorded_at}")
            return True
        else:
            print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ã™")
            return False

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        traceback.print_exc()
        return False

def download_from_s3(file_path: str, local_path: str) -> bool:
    """S3ã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        print(f"ğŸ“¥ S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {file_path}")
        s3_client.download_file(s3_bucket_name, file_path, local_path)
        print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {file_path}")
        return True
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == '404':
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        else:
            print(f"âŒ S3ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {error_code} - {str(e)}")
        return False
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def process_audio_for_passt(audio_data: np.ndarray, sample_rate: int) -> torch.Tensor:
    """
    éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’PaSSTç”¨ã«å‰å‡¦ç†

    âš ï¸ é‡è¦: PaSSTã¯32kHzã®éŸ³å£°ã‚’æœŸå¾…ã—ã¾ã™ï¼ˆv2ã®ASTã¯16kHzï¼‰

    Args:
        audio_data: éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        sample_rate: å…ƒã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ

    Returns:
        PaSSTç”¨ã«å‡¦ç†ã•ã‚ŒãŸTensor
    """
    # ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
    if len(audio_data.shape) > 1:
        audio_data = np.mean(audio_data, axis=1)

    # PaSSTãŒæœŸå¾…ã™ã‚‹32kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if sample_rate != SAMPLING_RATE:
        audio_data = librosa.resample(
            y=audio_data,
            orig_sr=sample_rate,
            target_sr=SAMPLING_RATE
        )
        print(f"ğŸ”„ ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {sample_rate}Hz â†’ {SAMPLING_RATE}Hz")

    # float32ã«å¤‰æ›
    if audio_data.dtype != np.float32:
        audio_data = audio_data.astype(np.float32)

    # æ­£è¦åŒ–ï¼ˆ-1.0 ã€œ 1.0ï¼‰
    max_val = np.max(np.abs(audio_data))
    if max_val > 0:
        audio_data = audio_data / max_val

    # Tensorã«å¤‰æ›ï¼ˆãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ ï¼‰
    audio_tensor = torch.from_numpy(audio_data).unsqueeze(0)

    return audio_tensor

def predict_audio_events(audio_tensor: torch.Tensor, top_k: int = 5,
                        threshold: float = 0.1) -> List[Dict]:
    """
    éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆã‚’äºˆæ¸¬

    Args:
        audio_tensor: PaSSTç”¨ã«å‡¦ç†æ¸ˆã¿ã®Tensor
        top_k: è¿”ã™ä¸Šä½äºˆæ¸¬ã®æ•°
        threshold: æœ€å°ç¢ºç‡ã—ãã„å€¤

    Returns:
        äºˆæ¸¬çµæœã®ãƒªã‚¹ãƒˆ
    """
    global model, device

    # ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
    audio_tensor = audio_tensor.to(device)

    # æ¨è«–å®Ÿè¡Œ
    with torch.no_grad():
        logits = model(audio_tensor)

    # Softmaxã§ç¢ºç‡ã«å¤‰æ›
    probs = torch.softmax(logits, dim=-1)

    # Top-kã®äºˆæ¸¬ã‚’å–å¾—
    top_probs, top_indices = torch.topk(probs[0], min(top_k, 527))

    # çµæœã‚’ãƒªã‚¹ãƒˆåŒ–
    predictions = []
    for idx, prob in zip(top_indices.cpu().numpy(), top_probs.cpu().numpy()):
        if prob >= threshold:
            class_id = str(int(idx))
            # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‹ã‚‰å–å¾—ï¼ˆãªã‘ã‚Œã°Class_XXXå½¢å¼ï¼‰
            label = labels_map.get(class_id, f"Class_{class_id}")
            predictions.append({
                "label": label,
                "score": round(float(prob), 4)
            })

    return predictions

def analyze_timeline(audio_data: np.ndarray, sample_rate: int,
                    segment_duration: float = 10.0,
                    overlap: float = 0.0,
                    top_k: int = 3,
                    threshold: float = 0.1) -> Dict:
    """
    éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§åˆ†æ

    Args:
        audio_data: éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        segment_duration: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é•·ã•ï¼ˆç§’ï¼‰- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ç§’ãŒæœ€é©
        overlap: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ç‡ (0-1) - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0ãŒæœ€é©
        top_k: å„æ™‚åˆ»ã§è¿”ã™ã‚¤ãƒ™ãƒ³ãƒˆæ•°
        threshold: æœ€å°ç¢ºç‡ã—ãã„å€¤

    Returns:
        æ™‚ç³»åˆ—åˆ†æçµæœ
    """
    # ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
    if len(audio_data.shape) > 1:
        audio_data = np.mean(audio_data, axis=1)

    # 32kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if sample_rate != SAMPLING_RATE:
        audio_data = librosa.resample(
            y=audio_data,
            orig_sr=sample_rate,
            target_sr=SAMPLING_RATE
        )
        sample_rate = SAMPLING_RATE

    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè¨­å®š
    segment_samples = int(segment_duration * sample_rate)
    hop_samples = int(segment_samples * (1 - overlap))

    # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³çµæœã‚’æ ¼ç´
    timeline = []
    all_events = {}

    # éŸ³å£°ãŒçŸ­ã„å ´åˆã¯å…¨ä½“ã‚’1ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨ã—ã¦å‡¦ç†
    if len(audio_data) < segment_samples:
        audio_tensor = torch.from_numpy(audio_data.astype(np.float32)).unsqueeze(0)
        events = predict_audio_events(audio_tensor, top_k, threshold)
        timeline.append({
            "time": 0.0,
            "events": events
        })
        for event in events:
            label = event["label"]
            if label not in all_events:
                all_events[label] = {"count": 0, "total_score": 0}
            all_events[label]["count"] += 1
            all_events[label]["total_score"] += event["score"]
    else:
        # é€šå¸¸ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†
        for i in range(0, len(audio_data) - segment_samples + 1, hop_samples):
            segment = audio_data[i:i + segment_samples]
            time_position = i / sample_rate

            # Tensorã«å¤‰æ›
            segment_tensor = torch.from_numpy(segment.astype(np.float32)).unsqueeze(0)

            # äºˆæ¸¬
            events = predict_audio_events(segment_tensor, top_k, threshold)

            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è¿½åŠ 
            timeline.append({
                "time": round(time_position, 1),
                "events": events
            })

            # ã‚¤ãƒ™ãƒ³ãƒˆã®é›†è¨ˆ
            for event in events:
                label = event["label"]
                if label not in all_events:
                    all_events[label] = {"count": 0, "total_score": 0}
                all_events[label]["count"] += 1
                all_events[label]["total_score"] += event["score"]

    # æœ€ã‚‚é »ç¹ãªã‚¤ãƒ™ãƒ³ãƒˆã‚’é›†è¨ˆ
    most_common = []
    for label, stats in sorted(all_events.items(), key=lambda x: x[1]["count"], reverse=True)[:5]:
        most_common.append({
            "label": label,
            "occurrences": stats["count"],
            "average_score": round(stats["total_score"] / stats["count"], 4)
        })

    return {
        "timeline": timeline,
        "summary": {
            "total_segments": len(timeline),
            "duration_seconds": round(len(audio_data) / sample_rate, 1),
            "segment_duration": segment_duration,
            "overlap": overlap,
            "most_common_events": most_common
        }
    }

async def process_single_file(file_path: str, threshold: float = 0.1, top_k: int = 3,
                             analyze_timeline_flag: bool = True,
                             segment_duration: float = 10.0,
                             overlap: float = 0.0) -> Dict:
    """
    å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã§ä¿å­˜ï¼‰
    """
    temp_file = None
    try:
        # audio_filesãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰recorded_atã‚’å–å¾—
        audio_file_response = supabase.table('audio_files') \
            .select('device_id, recorded_at') \
            .eq('file_path', file_path) \
            .single() \
            .execute()

        if not audio_file_response.data:
            return {"status": "error", "file_path": file_path, "error": "Audio file record not found"}

        device_id = audio_file_response.data['device_id']
        recorded_at = audio_file_response.data['recorded_at']

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å‡¦ç†ä¸­ã«æ›´æ–°
        await update_audio_files_status(file_path, 'processing')

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
            temp_file = tmp.name

        if not download_from_s3(file_path, temp_file):
            await update_audio_files_status(file_path, 'error')
            return {"status": "error", "file_path": file_path, "error": "Download failed"}

        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        audio_data, sample_rate = sf.read(temp_file)
        print(f"ğŸµ éŸ³å£°ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(audio_data)/sample_rate:.2f}ç§’, {sample_rate}Hz")

        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åˆ†æã‚’å®Ÿè¡Œ
        timeline_result = analyze_timeline(
            audio_data, sample_rate,
            segment_duration, overlap, top_k, threshold
        )

        # spot_featuresãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
        save_success = await save_to_spot_features(
            device_id,
            recorded_at,
            timeline_result['timeline']
        )

        if save_success:
            await update_audio_files_status(file_path, 'completed')
            return {
                "status": "success",
                "file_path": file_path,
                "device_id": device_id,
                "recorded_at": recorded_at,
                "timeline": timeline_result
            }
        else:
            await update_audio_files_status(file_path, 'error')
            return {"status": "error", "file_path": file_path, "error": "Save failed"}

    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        traceback.print_exc()
        await update_audio_files_status(file_path, 'error')
        return {"status": "error", "file_path": file_path, "error": str(e)}

    finally:
        if temp_file and os.path.exists(temp_file):
            os.remove(temp_file)

@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    load_model()

@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "PaSST Audio Event Detection API with Supabase Integration",
        "model": MODEL_NAME,
        "version": "3.0.0",
        "sampling_rate": f"{SAMPLING_RATE} Hz (32kHz)",
        "status": "ready" if model is not None else "not ready",
        "endpoints": {
            "/fetch-and-process-paths": "Process audio files from S3 via file paths",
            "/health": "Health check endpoint"
        }
    }

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "status": "healthy" if model is not None else "unhealthy",
        "model_loaded": model is not None,
        "model_name": MODEL_NAME,
        "sampling_rate": SAMPLING_RATE,
        "supabase_connected": supabase is not None,
        "s3_connected": s3_client is not None
    }

@app.post("/fetch-and-process-paths")
async def fetch_and_process_paths(request: FetchAndProcessPathsRequest):
    """
    file_pathsãƒ™ãƒ¼ã‚¹ã®éŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆv2å®Œå…¨äº’æ›ï¼‰

    Args:
        request: file_pathsé…åˆ—ã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
        å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ã¨è©³ç´°
    """
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    start_time = time.time()

    processed_files = []
    error_files = []

    print(f"ğŸš€ å‡¦ç†é–‹å§‹: {len(request.file_paths)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")

    for file_path in request.file_paths:
        result = await process_single_file(
            file_path,
            request.threshold,
            request.top_k,
            request.analyze_timeline,
            request.segment_duration,
            request.overlap
        )

        if result["status"] == "success":
            processed_files.append(file_path)
        else:
            error_files.append({
                "file_path": file_path,
                "error": result.get("error", "Unknown error")
            })

    execution_time = time.time() - start_time

    total_files = len(request.file_paths)
    success_count = len(processed_files)
    error_count = len(error_files)

    response = {
        "status": "success" if error_count == 0 else "partial",
        "summary": {
            "total_files": total_files,
            "processed": success_count,
            "errors": error_count
        },
        "processed_files": processed_files,
        "error_files": error_files if error_files else None,
        "execution_time_seconds": round(execution_time, 1),
        "message": f"{total_files}ä»¶ä¸­{success_count}ä»¶ã‚’æ­£å¸¸ã«å‡¦ç†ã—ã¾ã—ãŸ"
    }

    print(f"âœ… å‡¦ç†å®Œäº†: {success_count}/{total_files}ä»¶æˆåŠŸ (å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’)")

    return JSONResponse(content=response)

if __name__ == "__main__":
    print("=" * 50)
    print("PaSST Audio Event Detection API with Supabase")
    print(f"Model: {MODEL_NAME}")
    print(f"Sampling Rate: {SAMPLING_RATE} Hz (32kHz)")
    print("=" * 50)

    uvicorn.run(
        app,
        host="127.0.0.1",
        port=8017,
        log_level="info"
    )
